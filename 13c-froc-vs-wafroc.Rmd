# FROC vs. wAFROC {#froc-vs-wafroc}

---
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: R/learn/my_header.tex
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval.after = "fig.cap"
)

library(RJafroc)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(abind)
library(here)
library(dplyr)
```


```{r, echo = FALSE}
OpPtStr <- function(x,y) {
  y <- paste0("(", sprintf("%.3f", x), ", ", sprintf("%.3f", y), ")")
  return(y)
}
```


## TBA How much finished {#froc-vs-wafroc-how-much-finished}
50% 
Need to replace simulation values with analytical values


## Introduction {#froc-vs-wafroc-intro}

In the medical imaging context the FROC curve, which was introduced in [@bunch1977free], has been widely used for evaluating performance in the free-response paradigm, particularly in CAD algorithm development. Typically CAD researchers report sensitivity at a stated value of false positives per image, i.e., they report a *pair* of values. (TBA) From basic ROC analysis, see Section TBA (binary-task-model-beam-study), we know that a scalar FOM is preferable to reporting a pair of values. This chapter recommends adoption of the area under the wAFROC as the preferred scalar figure of merit in lieu of sensitivity / false positives per image pairs. operating characteristic in assessing performance in the free-response paradigm, and details simulation-based studies supporting this recommendation.

## FROC vs. wAFROC

Recall, from TBA Section (froc-paradigm-preview-rsm), that the RSM is defined by parameters $\mu, \lambda, \nu$ and $\zeta_1$. This section examines RSM-predicted TBA analytical FROC, wAFROC and ROC panels for two observers denoted R1 and R2. The former could be an algorithmic observer while the latter could be a radiologist. For typical threshold $\zeta_1$ parameters, three types of situations are considered: R2 has moderately better performance than R1, R2 has much better performance than R1 and R2 has slightly better performance than R1. For each type of simulation pairs of FROC, wAFROC and ROC curves are shown, one for each observer. Finally the simulations and panels are repeated for hypothetical R1 and R2 observers who report all suspicious regions, i.e., $\zeta_1 = -\infty$ for each observer. Both R1 and R2 observers share the same $\lambda, \nu$ parameters, and the only difference between them is in the $\mu$ and $\zeta_1$ parameters.   

```{r doOneFigure, echo=FALSE}
do_one_figure <- function(
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2) {
  set.seed(seed)
  Lk2 <- floor(runif(K2, 1, Lmax + 1))
  
  retABC <- CadVsRadPlots (
    mu1, 
    mu2, 
    lambda, 
    nu, 
    zeta1_1 = zeta1_1, 
    zeta1_2 = zeta1_2, 
    K1, 
    K2, 
    Lk2, 
    seed)
  
  froc_plot_A <- retABC$froc$Plot + labs(tag = "A")
  wafroc_plot_B <- retABC$wafroc$Plot + labs(tag = "B")
  roc_plot_C <- retABC$roc$Plot + labs(tag = "C")
  wafroc_1_B <- retABC$wafroc1
  wafroc_2_B <- retABC$wafroc2
  roc_1_C <- retABC$roc1
  roc_2_C <- retABC$roc2
  
  retDEF <- CadVsRadPlots (
    mu1, 
    mu2, 
    lambda, 
    nu, 
    zeta1_1 = -Inf, 
    zeta1_2 = -Inf, 
    K1, 
    K2, 
    Lk2, 
    seed)
  
  froc_plot_D <- retDEF$froc$Plot + labs(tag = "D")
  wafroc_plot_E <- retDEF$wafroc$Plot + labs(tag = "E")
  roc_plot_F <- retDEF$roc$Plot + labs(tag = "F")
  wafroc_1_E <- retDEF$wafroc1
  wafroc_2_E <- retDEF$wafroc2
  roc_1_F <- retDEF$roc1
  roc_2_F <- retDEF$roc2
  
  return(list(
    froc_plot_A = froc_plot_A,
    wafroc_plot_B = wafroc_plot_B,
    roc_plot_C = roc_plot_C,
    froc_plot_D = froc_plot_D,
    wafroc_plot_E = wafroc_plot_E,
    roc_plot_F = roc_plot_F,
    wafroc_1_B = wafroc_1_B,
    wafroc_2_B = wafroc_2_B,
    roc_1_C = roc_1_C,
    roc_2_C = roc_2_C,
    wafroc_1_E = wafroc_1_E,
    wafroc_2_E = wafroc_2_E,
    roc_1_F = roc_1_F,
    roc_2_F = roc_2_F
  ))
}
```


### Moderate difference in performance

```{r fig1, cache = FALSE, attr.source = ".numberLines"}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1.0
mu2 <- 1.5
zeta1_1 <- -1
zeta1_2 <- 1.5
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_1A <- ret$froc_plot_A
wafroc_plot_1B <- ret$wafroc_plot_B
roc_plot_1C <- ret$roc_plot_C
froc_plot_1D <- ret$froc_plot_D
wafroc_plot_1E <- ret$wafroc_plot_E
roc_plot_1F <- ret$roc_plot_F
wafroc_1_1B <- ret$wafroc_1_B
wafroc_2_1B <- ret$wafroc_2_B
roc_1_1C <- ret$roc_1_C
roc_2_1C <- ret$roc_2_C
wafroc_1_1E <- ret$wafroc_1_E
wafroc_2_1E <- ret$wafroc_2_E
roc_1_1F <- ret$roc_1_F
roc_2_1F <- ret$roc_2_F
```

The $\lambda$ and $\nu$ parameters are defined at lines 3 and 4 of the preceding code: $\lambda = \nu = 1$. The number of simulated cases is defined, lines 5-6, by $K_1 = 500$ and $K_2 = 700$. The simulated R1 observer $\mu$ parameter is defined at line 7 by $\mu_{1} = 1$ and that of the simulated R2 observer is defined at line 8 by $\mu_{2} = 1.5$. Based on these choices one expect R2 to be moderately better than R1. The corresponding threshold parameters are (lines 9 -10) $\zeta_{1} = -1$ for R1 and $\zeta_{1} = 1.5$ for R2. The maximum number of lesions per case is defined at line 11 by `Lmax` = 2. The actual number of lesions per case is determined determined by random sampling within the helper function `do_one_figure()` called at lines 14-16. This function returns a large list `ret`, whose contents are as follows:

* `ret$froc_plot_A`: a pair of FROC panels for the thresholds specified above, a red panel labeled "R: 1" corresponding to R1 and a blue panel labeled "R: 2" corresponding to R2. These are shown in panel A.
* `ret$wafroc_plot_B`: a pair of wAFROC panels, similarly labeled. These are shown in panel B.
* `ret$roc_plot_C`: a pair of ROC panels, similarly labeled. These are shown in panel C.
* `ret$froc_plot_D`: a pair of FROC panels for the both thresholds at $-\infty$. These are shown in panel D.
* `ret$froc_plot_E`: a pair of wAFROC panels for the both thresholds at $-\infty$. These are shown in panel E.
* `ret$froc_plot_F`: a pair of ROC panels for the both thresholds at $-\infty$. These are shown in panel F.
* `ret$wafroc_1_B`: the wAFROC AUC for R1, i.e., the area under the curve labeled "R: 1" in panel B.
* `ret$wafroc_2_B`: the wAFROC AUC for R2, i.e., the area under the curve labeled "R: 2" in panel B.
* `ret$roc_1_C`: the ROC AUC for R1, i.e., the area under the curve labeled "R: 1" in panel C.
* `ret$roc_2_C`: the ROC AUC for R2, i.e., the area under the curve labeled "R: 2" in panel C.
* `ret$wafroc_1_E`: the wAFROC AUC for R1, i.e., the area under the curve labeled "R: 1" in panel E.
* `ret$wafroc_2_E`: the wAFROC AUC for R2, i.e., the area under the curve labeled "R: 2" in panel E.
* `ret$roc_1_F`: the ROC AUC for R1, i.e., the area under the curve labeled "R: 1" in panel F.
* `ret$roc_2_F`: the ROC AUC for R2, i.e., the area under the curve labeled "R: 2" in panel F.


```{r extractEndPts1, echo=FALSE}
# extract coordinates of end-point
# the 1 refers to fig. 1
nlf_1_1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 1"]) 
llf_1_1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: 1"]) 
nlf_2_1A <- max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"]) 
llf_2_1A <- max(froc_plot_1A$data$genOrdinate[froc_plot_1A$data$Reader == "R: 2"]) 

nlf_1_1D <- max(froc_plot_1D$data$genAbscissa[froc_plot_1D$data$Reader == "R: 1"]) 
llf_1_1D <- max(froc_plot_1D$data$genOrdinate[froc_plot_1D$data$Reader == "R: 1"]) 
nlf_2_1D <- max(froc_plot_1D$data$genAbscissa[froc_plot_1D$data$Reader == "R: 2"]) 
llf_2_1D <- max(froc_plot_1D$data$genOrdinate[froc_plot_1D$data$Reader == "R: 2"]) 
```



```{r froc-vs-wafroc-plot1, fig.cap="Plots A and D: FROC curves for the R1 and R2 observers; B and E are corresponding wAFROC curves and C and F are corresponding ROC curves. All curves in this plot are for $\\lambda = \\nu = 1$. All RAD_1 curves are for $\\mu = 1$ and all RAD_2 curves are for $\\mu = 1.5$. For panels A, B and C, $\\zeta_1 = -1$ for R1 and $\\zeta_1 = 1.5$ for R2. For panels D, E and F, $\\zeta_1 = -\\infty$ for R1 and R2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_1A,wafroc_plot_1B,roc_plot_1C,froc_plot_1D,wafroc_plot_1E,roc_plot_1F,nrow=2,ncol=3)
```


The coordinates of the end-point of the R1 FROC in panel A are `r OpPtStr(nlf_1_1A, llf_1_1A)`. Those of the R2 FROC curve in A are `r OpPtStr(nlf_2_1A, llf_2_1A)`. The FROC for the R1 observer extends to much larger NLF values while that for the R2 observer is relatively short and steep. One suspects the R2 observer is performing better than R1: he is better at finding lesions and producing fewer NLs, both of which are desirable characteristics, but he is adopting a too-strict reporting criterion. If he could be induced to relax the threshold and report more NLs, his LLF would exceed that of the R1 observer while still maintaining a lower NLF. However, as this involves a subjective extrapolation, it is not possible to objectively quantify this from the FROC curves. The basic issue is the lack of a common NLF range for the two panels. If a common NLF range is "forced", for example defined as the common NLF range 0 to `r sprintf("%.4f", max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"]))`, where both curves contribute, it would ignore most NLs from the R1 observer.

Algorithm developers typically quote LLF at a specified NLF. According to the two panels in A, the R2 observer is better if the NLF value is chosen to less than `r sprintf("%.4f", max(froc_plot_1A$data$genAbscissa[froc_plot_1A$data$Reader == "R: 2"]))` - this is the maximum NLF value for the R2 curve in A - but there is no basis for comparison for larger values of NLF (because the R2 observer does not provide any data beyond the observed end-point). A similar problem was encountered in ROC analysis when comparing a pair of sensitivity-specificity values, where, given differing choices of thresholds, ambiguous results can be obtained, see Section TBA (binary-task-model-beam-study). Indeed, this was the rationale for using AUC under the ROC curve as an unambiguous measure of performance.

Plot B shows wAFROC curves for the same datasets whose FROC curves are shown in panel A. **The wAFROC is contained within the unit square, a highly desirable characteristic, which solves the lack of a common NLF range problem with the FROC.** The wAFROC AUC under the R2 observer is visibly greater than that for the R1 observer, even though -- due to his higher threshold -- his AUC estimate is actually biased downward (because the R2 observer is adopting a high threshold, his $\text{LLF}_{\text{max}}$ is smaller than it would have been with a lower threshold, and consequently the area under the large straight line segment from the uppermost non-trivial operating point to (1,1) is smaller). AUCs under the two wAFROC panels in B are `r sprintf("%.4f", wafroc_1_1B)` for R1 and `r sprintf("%.4f", wafroc_2_1B)` for R2.


Plot C shows ROC curves. Since the curves cross, it is not clear which has the larger AUC. AUCs under the two curves in C are `r sprintf("%.4f", roc_1_1C)` for R1 and `r sprintf("%.4f", roc_2_1C)` for R2, which are close, but here is an example where the ordering given by the wAFROC is opposite to that given by the ROC. 

Plots D, E and F correspond to A, B and C with this important difference: the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the R1 FROC in panel D are `r OpPtStr(nlf_1_1D, llf_1_1D)`. Those of the R2 FROC in panel D are `r OpPtStr(nlf_2_1D, llf_2_1D)`. The R2 observer has higher LLF at lower NLF, and there can be no doubt that he is better. Panels E and F confirm that R2 is actually the better observer *over the entire FPF range*. AUCs under the two wAFROC curves in E are `r sprintf("%.4f", wafroc_1_1E)` for R1 and `r sprintf("%.4f", wafroc_2_1E)` for R2. AUCs under the two ROC curves in F are `r sprintf("%.4f",roc_1_1F)` for R1 and `r sprintf("%.4f", roc_2_1F)` for R2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. This is because the probability of a lesion localization (case is declared positive *and* a lesion is correctly localized) is smaller than the probability of a true positive (case is declared positive). In other words, the ROC is everywhere above the wAFROC.


### Large difference in performance

```{r fig2, cache = FALSE, echo=FALSE}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1
mu2 <- 2 # large difference
zeta1_1 <- -1
zeta1_2 <- 2 # high cutoff
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_2A <- ret$froc_plot_A
wafroc_plot_2B <- ret$wafroc_plot_B
roc_plot_2C <- ret$roc_plot_C
froc_plot_2D <- ret$froc_plot_D
wafroc_plot_2E <- ret$wafroc_plot_E
roc_plot_2F <- ret$roc_plot_F
# Notation
# wafroc fom for reader figure panel
# e.g. wafroc_1_2B
# wafroc fom for reader1 figure2 panelB
wafroc_1_2B <- ret$wafroc_1_B
wafroc_2_2B <- ret$wafroc_2_B
roc_1_2C <- ret$roc_1_C
roc_2_2C <- ret$roc_2_C
wafroc_1_2E <- ret$wafroc_1_E
wafroc_2_2E <- ret$wafroc_2_E
roc_1_2F <- ret$roc_1_F
roc_2_2F <- ret$roc_2_F
```


```{r extractEndPts2, echo=FALSE}
# extract coordinates of end-point
# nlf_1_2A = nlf_rdr1_fig2panelA
nlf_1_2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: 1"]) 
llf_1_2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 1"]) 
nlf_2_2A <- max(froc_plot_2A$data$genAbscissa[froc_plot_2A$data$Reader == "R: 2"]) 
llf_2_2A <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 2"]) 

nlf_1_2D <- max(froc_plot_2D$data$genAbscissa[froc_plot_2D$data$Reader == "R: 1"]) 
llf_1_2D <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 1"]) 
nlf_2_2D <- max(froc_plot_2D$data$genAbscissa[froc_plot_2D$data$Reader == "R: 2"]) 
llf_2_2D <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 2"]) 

x <- wafroc_plot_2B$data$genAbscissa[wafroc_plot_2B$data$Reader == "R: 2"];fpfRad2B <- x[length(x)-1]
x <- wafroc_plot_2E$data$genAbscissa[wafroc_plot_2E$data$Reader == "R: 2"];fpfRad2E <- x[length(x)-1]
llf_2_2B <- max(froc_plot_2A$data$genOrdinate[froc_plot_2A$data$Reader == "R: 2"]) # llf is identical to that of A
llf_2_2E <- max(froc_plot_2D$data$genOrdinate[froc_plot_2D$data$Reader == "R: 2"]) # llf is identical to that of D
```

```{r froc-vs-wafroc-plot2, fig.cap="Similar to preceding figure but with the following changes. All RAD_2 curves are for $\\mu = 2$ and for panels A, B and C $\\zeta_1 = 2$ for R2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_2A,wafroc_plot_2B,roc_plot_2C,froc_plot_2D,wafroc_plot_2E,roc_plot_2F,nrow=2,ncol=3)
```


In Fig. \@ref(fig:froc-vs-wafroc-plot2) panel A, the R1 parameters are the same as in Fig. \@ref(fig:froc-vs-wafroc-plot1), but the R2 parameters are $\mu_{2} = 2$ and $\zeta_1 = +2$. Doubling the separation parameter over that of R1 ($\mu_{1} = 1$) has a huge effect on performance. The end-point coordinates of the FROC for R1 are `r OpPtStr(nlf_1_2A, llf_1_2A)`. The end-point coordinates of the FROC for R2 are `r OpPtStr(nlf_2_2A, llf_2_2A)`. The common NLF region defined by NLF = 0 to NLF = `r sprintf("%.4f", nlf_2_2A)` *would exclude almost all of the marks made by R1*. The wAFROC panels in panel B show the markedly greater performance of R2 over R1 (the AUCs are `r sprintf("%.4f", wafroc_1_2B)` for R1 and `r sprintf("%.4f", wafroc_2_2B)` for R2). The inter-reader difference is larger (compared to Fig. \@ref(fig:froc-vs-wafroc-plot1) panel B), despite the greater downward bias working against the R2 observer. Panel C shows ROC panels for the two observers. Although the curves cross, it is evident that R2 has the greater AUC. The AUCs are `r sprintf("%.4f", roc_1_2C)` for R1 and `r sprintf("%.4f", roc_2_2C)` for R2.

Plots D, E and F correspond to A, B and C with the difference that the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the R1 FROC in panel D are OpPtStr(nlf_1_2D, llf_1_2D)`. Those of the R2 FROC in panel D are OpPtStr(nlf_2_2D, llf_2_2D)`. The R2 observer has higher LLF at lower NLF, and there can be no doubt that he is better. Panels E and F confirm that R2 is actually the better observer *over the entire FPF range*. AUCs under the two wAFROC curves in E are `r sprintf("%.4f", wafroc_1_2E)` for R1 and `r sprintf("%.4f", wafroc_2_2E)` for R2. AUCs under the two ROC curves in F are `r sprintf("%.4f", roc_1_2F)` for R1 and `r sprintf("%.4f", roc_2_2F)` for R2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. 

### Small difference in performance and identical thresholds
```{r fig3, cache = FALSE, echo=FALSE}
source(here("R/CH13-CadVsRadPlots/CadVsRadPlots.R"))

nu <- 1
lambda <- 1
K1 <- 500
K2 <- 700
mu1 <- 1.0
mu2 <- 1.1
zeta1_1 <- -1
zeta1_2 <- -1
Lmax <- 2
seed <- 1

ret <- do_one_figure (
  seed, Lmax, mu1, 
  mu2, lambda, nu, zeta1_1, zeta1_2, K1, K2)

froc_plot_3A <- ret$froc_plot_A
wafroc_plot_3B <- ret$wafroc_plot_B
roc_plot_3C <- ret$roc_plot_C
froc_plot_3D <- ret$froc_plot_D
wafroc_plot_3E <- ret$wafroc_plot_E
roc_plot_3F <- ret$roc_plot_F
wafroc_1_3B <- ret$wafroc_1_B
wafroc_2_3B <- ret$wafroc_2_B
roc_1_3C <- ret$roc_1_C
roc_2_3C <- ret$roc_2_C
wafroc_1_3E <- ret$wafroc_1_E
wafroc_2_3E <- ret$wafroc_2_E
roc_1_3F <- ret$roc_1_F
roc_2_3F <- ret$roc_2_F
```



```{r extractEndPts3, echo=FALSE}
# extract coordinates of end-point
nlf_1_3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: 1"]) 
llf_1_3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: 1"]) 
nlf_2_3A <- max(froc_plot_3A$data$genAbscissa[froc_plot_3A$data$Reader == "R: 2"]) 
llf_2_3A <- max(froc_plot_3A$data$genOrdinate[froc_plot_3A$data$Reader == "R: 2"]) 

nlf_1_3D <- max(froc_plot_3D$data$genAbscissa[froc_plot_3D$data$Reader == "R: 1"]) 
llf_1_3D <- max(froc_plot_3D$data$genOrdinate[froc_plot_3D$data$Reader == "R: 1"]) 
nlf_2_3D <- max(froc_plot_3D$data$genAbscissa[froc_plot_3D$data$Reader == "R: 2"]) 
llf_2_3D <- max(froc_plot_3D$data$genOrdinate[froc_plot_3D$data$Reader == "R: 2"]) 
```


```{r froc-vs-wafroc-plot3, fig.cap="Similar to preceding figure but with the following changes. All RAD_2 curves are for $\\mu = 1.1$ and for panels A, B and C, $\\zeta_1 = -1$ for R2.", fig.show='hold', echo=FALSE}
grid.arrange(froc_plot_3A,wafroc_plot_3B,roc_plot_3C,froc_plot_3D,wafroc_plot_3E,roc_plot_3F,nrow=2,ncol=3)
```


The final example, Fig. \@ref(fig:froc-vs-wafroc-plot3) shows that *when there is a small difference in performance*, there is less ambiguity in using the FROC as a basis for measuring performance. The R1 parameters are the same as in Fig. \@ref(fig:froc-vs-wafroc-plot1) but the R2 parameters are $\mu = 1.1$ and $\zeta_1= -1$. In other words, the $\mu$ parameter is 10% larger and the thresholds are identical. This time there is much more common NLF range overlap in panel A and one is counting most of the marks for the R1 reader. The end-point coordinates of the FROC for R1 are `r OpPtStr(nlf_1_3A, llf_1_3A)`. The end-point coordinates of the FROC for R2 are (`r OpPtStr(nlf_2_3A, llf_2_3A)`. The common NLF region defined by NLF = 0 to NLF = `r sprintf("%.4f", nlf_2_3A)` includes almost all of the marks made by R1. The wAFROC panels in panel B show the slight greater performance of R2 over R1 (the AUCs are `r sprintf("%.4f", wafroc_1_3B)` for R1 and `r sprintf("%.4f", wafroc_2_3B)` for R2). Panel C shows ROC panels for the two observers. Although the curves cross, it is evident that R2 has the greater AUC. The AUCs are `r sprintf("%.4f", roc_1_2C)` for R1 and `r sprintf("%.4f", roc_2_2C)` for R2.

Plots D, E and F correspond to A, B and C with the difference that the two threshold parameters are set to $-\infty$. The coordinates of the end-point of the R1 FROC in panel D are (`r OpPtStr(nlf_1_3D, llf_1_3D)`. Those of the R2 FROC in panel D are (`r OpPtStr(nlf_2_3D, llf_2_3D)`. Panels E and F confirm that R2 is actually the better observer over the entire FPF range. AUCs under the two wAFROC curves in E are `r sprintf("%.4f", wafroc_1_3E)` for R1 and `r sprintf("%.4f", wafroc_2_3E)` for R2. AUCs under the two ROC curves in F are `r sprintf("%.4f", roc_1_3F)` for R1 and `r sprintf("%.4f", roc_2_3F)` for R2. These confirm the visual impressions of panels in panels E and F. Notice that each ROC AUC is larger than the corresponding wAFROC AUC. 

## Summary of simulations

The following tables summarize the numerical values from the plots in this chapter. Table \@ref(tab:froc-vs-wafroc-summary-table-rdr1) refers to the R1 observer, and Table \@ref(tab:froc-vs-wafroc-summary-table-rdr2) refers to the R2 observer.

### Summary of R1 simulations


```{r rdr1, echo=FALSE}
dig <- 4

#cell1 <- paste0("(", as.character(format(nlf_1_1A, digits = dig)), ", ", as.character(format(llf_1_1A, digits = dig)), ")")
#cell2 <- paste0("(", as.character(format(nlf_1_1D, digits = dig)), ", ", as.character(format(llf_1_1D, digits = dig)), ")")
cell1 <- as.character(format(wafroc_1_1B, digits = dig))
cell2 <- as.character(format(wafroc_1_1E, digits = dig))
cell3 <- as.character(format(roc_1_1C, digits = dig))
cell4 <- as.character(format(roc_1_1F, digits = dig))
```



```{r froc-vs-wafroc-summary-table-rdr1, echo=FALSE}
tableCells = array(dim = c(1,4))

tableCells[1, 1]  <- cell1
tableCells[1, 2]  <- cell2
tableCells[1, 3]  <- cell3
tableCells[1, 4]  <- cell4

df <- as.data.frame(tableCells)
colnames(df) <- c("wAFROC-B", "wAFROC-E", "ROC-C", "ROC-F")
knitr::kable(df, caption = "Summary of R1 simulations: A refers to panel A, B refers to panel B, etc.", escape = FALSE)
```


-   The first column is labeled "wAFROC-B", meaning the R1 wAFROC AUC in panel B, which are identical for the three figures (one may visually confirm that the red curves in panels A, B ad C in the three figures are identical; likewise for the red curves in panels D, E and F).
-   The second column is labeled "wAFROC-E", meaning the R1 wAFROC AUC in panel E, which are identical for the three figures.
-   The third column is labeled "ROC-C", meaning the R1 ROC AUC in panel C, which are identical for the three figures.
-   The fourth column is labeled "ROC-F", meaning the R1 ROC AUC in panel F, which are identical for the three figures.


### Summary of R2 simulations
```{r rdr2, echo=FALSE}

cell_11 <- "1"
#cell_12 <- paste0("(", as.character(format(nlf_2_1A, digits = dig)), ", ", as.character(format(llf_2_1A, digits = dig)), ")")
#cell_13 <- paste0("(", as.character(format(nlf_2_1D, digits = dig)), ", ", as.character(format(llf_2_1D, digits = dig)), ")")
cell_12 <- as.character(format(wafroc_2_1B, digits = dig))
cell_13 <- as.character(format(wafroc_2_1E, digits = dig))
cell_14 <- as.character(format(roc_2_1C, digits = dig))
cell_15 <- as.character(format(roc_2_1F, digits = dig))

cell_21 <- "2"
#cell_22 <- paste0("(", as.character(format(nlf_2_2A, digits = dig)), ", ", as.character(format(llf_2_2A, digits = dig)), ")")
#cell_23 <- paste0("(", as.character(format(nlf_2_2D, digits = dig)), ", ", as.character(format(llf_2_2D, digits = dig)), ")")
cell_22 <- as.character(format(wafroc_2_2B, digits = dig))
cell_23 <- as.character(format(wafroc_2_2E, digits = dig))
cell_24 <- as.character(format(roc_2_2C, digits = dig))
cell_25 <- as.character(format(roc_2_2F, digits = dig))

cell_31 <- "3"
#cell_32 <- paste0("(", as.character(format(nlf_2_3A, digits = dig)), ", ", as.character(format(llf_2_3A, digits = dig)), ")")
#cell_33 <- paste0("(", as.character(format(nlf_2_3D, digits = dig)), ", ", as.character(format(llf_2_3D, digits = dig)), ")")
cell_32 <- as.character(format(wafroc_2_3B, digits = dig))
cell_33 <- as.character(format(wafroc_2_3E, digits = dig))
cell_34 <- as.character(format(roc_2_3C, digits = dig))
cell_35 <- as.character(format(roc_2_3F, digits = dig))
```


```{r froc-vs-wafroc-summary-table-rdr2, echo=FALSE}
tableCells = array(dim = c(3,5))

tableCells[1,]  <- c(cell_11, cell_12, cell_13, cell_14, cell_15)#, cell_16, cell_17)
tableCells[2,]  <- c(cell_21, cell_22, cell_23, cell_24, cell_25)#, cell_26, cell_27)
tableCells[3,]  <- c(cell_31, cell_32, cell_33, cell_34, cell_35)#, cell_36, cell_37)
                     
df <- as.data.frame(tableCells)
rownames(df) <- c("1","2","3")
colnames(df) <- c("Fig", "wAFROC-B", "wAFROC-E", "ROC-C", "ROC-F")
knitr::kable(df, caption = "Summary of R2 simulations: Fig refers to the figure number in this chapter, A refers to panel A, B refers to panel B, etc.", escape = FALSE)
```

-   The first column refers to the figure number, for example, "1" refers to Fig. \@ref(fig:froc-vs-wafroc-plot1), "2" refers to Fig. \@ref(fig:froc-vs-wafroc-plot2), and "3" refers to Fig. \@ref(fig:froc-vs-wafroc-plot3).
-   The second column is labeled "wAFROC-B", meaning the R2 wAFROC AUC corresponding to the blue curve in panel B.
-   The third column is labeled "wAFROC-E", meaning the R2 wAFROC AUC corresponding to the blue curve in panel E.
-   The fourth column is labeled "ROC-C", meaning the R2 ROC AUC corresponding to the blue curve in panel C.
-   The fifth column is labeled "ROC-F", meaning the R2 ROC AUC corresponding to the blue curve in panel F.

### Comments {#froc-vs-wafroc-comments}

-   For the same figure label the R1 panels are identical in the three figures. This is the reason why Table \@ref(tab:froc-vs-wafroc-summary-table-rdr1) has only one row. A *fixed* R1 dataset is being compared to *varying* R2 datasets.
-   The first R2 dataset, Fig. \@ref(fig:froc-vs-wafroc-plot1) A, B or C, might be considered representative of an average radiologist, the second one, Fig. \@ref(fig:froc-vs-wafroc-plot2) A, B or C, is a super-expert and the third one, Fig. \@ref(fig:froc-vs-wafroc-plot3) A, B or C, is only nominally better than R1.
-   Plots D, E and F are for hypothetical R1 and R2 observers that report *all* suspicious regions. The differences between A and D are minimal for the R1 observer, but marked for the R2 observer. Likewise for the differences between B and E.


## Effect size comparison {#froc-vs-wafroc-effect-sizes}

* The effect size is defined as the AUC -- calculated using either wAFROC or ROC -- difference between RDR-2 and RDR-1 for the same figure. For example, for Fig. \@ref(fig:froc-vs-wafroc-plot2) and the wAFROC AUC effect size, one takes the difference between the AUCs under the R2 (blue) minus R1 (red) curves in panel B. 
* In all three figures the wAFROC effect size (ES) is larger than the corresponding ROC effect size. 
* For Fig. \@ref(fig:froc-vs-wafroc-plot1) panels B and C:
   + The wAFROC effect size is `r sprintf("%.4f", (wafroc_2_1B - wafroc_1_1B))`, 
   + The ROC effect size is `r sprintf("%.4f", (roc_2_1C - roc_1_1C))`. 
* For Fig. \@ref(fig:froc-vs-wafroc-plot2) panels B and C: 
   + The wAFROC effect size is `r sprintf("%.4f", (wafroc_2_2B - wafroc_1_2B))`, 
   + The ROC effect size is `r sprintf("%.4f", (roc_2_2C - roc_1_2C))`. 
* For Fig. \@ref(fig:froc-vs-wafroc-plot3) panels B and C: 
   + The wAFROC effect size is `r sprintf("%.4f", (wafroc_2_3B - wafroc_1_3B))`, 
   + The ROC effect size is `r sprintf("%.4f", (roc_2_3C - roc_1_3C))`. 


These results are summarized in Table \@ref(tab:froc-vs-wafroc-effect-size-rdr2). 

Since effect size enters as the *square* in sample size formulas, wAFROC yields greater statistical power than ROC. The "small difference" example, corresponding to row number 2, is more typical of modality comparison studies where the modalities being compared are only slightly different. In this case the wAFROC effect size is about twice the corresponding ROC value - see chapter on FROC sample size TBA.



```{r froc-vs-wafroc-effect-size-rdr2, echo=FALSE}
tableCells = array(dim = c(3,3))

# notation
# reader figure panel
tableCells[1, 1]  <- cell_11
tableCells[1, 2]  <- as.character(format(wafroc_2_1B - wafroc_1_1B, digits = dig))
tableCells[1, 3]  <- as.character(format(roc_2_1C - roc_1_1C, digits = dig))

tableCells[2, 1]  <- cell_21
tableCells[2, 2]  <- as.character(format(wafroc_2_2B - wafroc_1_2B, digits = dig))
tableCells[2, 3]  <- as.character(format(roc_2_2C - roc_1_2C, digits = dig))

tableCells[3, 1]  <- cell_31
tableCells[3, 2]  <- as.character(format(wafroc_2_3B - wafroc_1_3B, digits = dig))
tableCells[3, 3]  <- as.character(format(roc_2_3C - roc_1_3C, digits = dig))

df <- as.data.frame(tableCells)
colnames(df) <- c("Fig", "ES-wAFROC", "ES-ROC")
knitr::kable(df, caption = "Effect size comparions for R1 simulations: Fig refers to the figure number in this chapter.", escape = FALSE)
```



## Performance depends on $\zeta_1$ {#froc-vs-wafroc-peformance-depends-on-zeta1}
Consider the wAFROC AUCs for the R2 curves in Fig. \@ref(fig:froc-vs-wafroc-plot2) panels B and E. The wAFROC AUC for R2 in panel B is `r sprintf("%.4f", wafroc_2_2B)` while that for R2 in panel E is `r sprintf("%.4f", wafroc_2_2E)`. The only difference between the simulation parameters for the two curves are $\zeta_1 = 2$ for panel B and $\zeta_1 = -\infty$ for panel E. Clearly wAFROC AUC depends on the value of $\zeta_1$. 

A similar result applies when considering the ROC curves in Fig. \@ref(fig:froc-vs-wafroc-plot2) panels C and F. The ROC AUC for R2 in panel C is `r sprintf("%.4f", roc_2_2C)` while that for R2 in panel F is `r sprintf("%.4f", roc_2_2F)`. Clearly ROC AUC also depends on the value of $\zeta_1$. 

The reason is that in panels B and C the respective AUCs are depressed due to high value of threshold parameter. The (very good) radiologist is seriously under-reporting and choosing to operate near the origin of a steep wAFROC/ROC curve. It as as if in an ROC study the reader is giving too much importance to specificity and therefore not achieving higher sensitivity.

*Since performance depends on threshold, this opens up the possibility of optimizing performance by finding the threshold that maximizes AUC. This is the subject of the next chapter.*

## Discussion {#froc-vs-wafroc-Discussion}

## References {#froc-vs-wafroc-references}
